import os
import csv
import joblib
import numpy as np
import torch
import torch.nn.functional as F
import cv2
import tkinter as tk
from tkinter import filedialog
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor

# sklearn æœºå™¨å­¦ä¹ åº“
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, accuracy_score, f1_score

# å¼•å…¥æ ¸å¿ƒæ¨¡å—
from detect_birds_single_maskenabled import BirdDetector, load_best_available_image

# ================= é…ç½®åŒºåŸŸ =================
ROI_STANDARD_SIZE = 1600
BATCH_SIZE = 16       # æ˜¾å­˜å…è®¸çš„æƒ…å†µä¸‹è¶Šå¤§è¶Šå¥½
NUM_WORKERS = 8       # CPU çº¿ç¨‹æ•°
CLASS_NAMES = ['Trash', 'Soft', 'Perfect']
# ===========================================

class GPUBatchFeatureExtractor:
    """
    [æ ¸å¿ƒç»„ä»¶] GPU æ‰¹é‡ç‰¹å¾æå–å™¨
    å®Œå…¨å¤åˆ»ä¸»ç¨‹åºé€»è¾‘ï¼Œç¡®ä¿è®­ç»ƒ/æ¨ç†ä¸€è‡´æ€§
    """
    def __init__(self):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"âš¡ ç‰¹å¾æå–å¼•æ“: {self.device.upper()} (Batch Mode)")
        
        self.sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3).to(self.device)
        self.sobel_y = torch.tensor([[1, 2, 1], [0, 0, 0], [-1, -2, -1]], dtype=torch.float32).view(1, 1, 3, 3).to(self.device)
        self.freq_masks_cache = {}

    def _prepare_tensor_batch(self, roi_list):
        batch_tensors = []
        target_h, target_w = ROI_STANDARD_SIZE, ROI_STANDARD_SIZE
        
        for roi in roi_list:
            if roi is None:
                tensor = torch.zeros((1, target_h, target_w), dtype=torch.float32)
            else:
                tensor = torch.from_numpy(roi).float()
                if len(tensor.shape) == 3:
                    tensor = 0.114 * tensor[:,:,0] + 0.587 * tensor[:,:,1] + 0.299 * tensor[:,:,2]
                
                h, w = tensor.shape
                pad_h, pad_w = target_h - h, target_w - w
                tensor = F.pad(tensor.unsqueeze(0), (0, pad_w, 0, pad_h), "constant", 0)
            
            batch_tensors.append(tensor)
        return torch.stack(batch_tensors).to(self.device)

    def extract_batch(self, roi_list):
        if not roi_list: return []
        batch_size = len(roi_list)
        
        # 1. é¢„å¤„ç†
        input_tensor = self._prepare_tensor_batch(roi_list)
        
        # 2. æ©è†œä¸è…èš€
        mask_binary = (input_tensor > 1.0).float()
        erode_ksize = int(ROI_STANDARD_SIZE * 0.01) | 1
        pad = erode_ksize // 2
        inner_mask = -F.max_pool2d(-mask_binary, kernel_size=erode_ksize, stride=1, padding=pad)
        
        # 3. æ¢¯åº¦
        gx = F.conv2d(input_tensor, self.sobel_x, padding=1)
        gy = F.conv2d(input_tensor, self.sobel_y, padding=1)
        magnitude = torch.sqrt(gx**2 + gy**2)
        
        # 4. FFT
        fft_res = torch.fft.fft2(input_tensor)
        fft_shift = torch.fft.fftshift(fft_res)
        mag_spec = 20 * torch.log(torch.abs(fft_shift) + 1)
        
        features_batch = []
        
        # é¢„è®¡ç®—æ©è†œ
        H, W = ROI_STANDARD_SIZE, ROI_STANDARD_SIZE
        if (H, W) not in self.freq_masks_cache:
            cy, cx = H // 2, W // 2
            y = torch.arange(H, device=self.device).view(-1, 1) - cy
            x = torch.arange(W, device=self.device).view(1, -1) - cx
            dist_sq = x**2 + y**2
            max_r_sq = min(cy, cx)**2
            self.freq_masks_cache[(H, W)] = (dist_sq, max_r_sq)
            
        dist_sq, max_r_sq = self.freq_masks_cache[(H, W)]
        
        # 5. æå–ç»Ÿè®¡å€¼
        for i in range(batch_size):
            if roi_list[i] is None:
                features_batch.append(None)
                continue
                
            curr_mag = magnitude[i, 0]
            curr_mask = inner_mask[i, 0]
            curr_spec = mag_spec[i, 0]
            
            valid_grads = curr_mag[curr_mask > 0.5]
            if valid_grads.numel() == 0:
                features_batch.append([0]*6)
                continue
            
            feat_peak = torch.quantile(valid_grads, 0.95).item()
            feat_mean = torch.mean(valid_grads).item()
            feat_std = torch.std(valid_grads).item()
            
            def get_energy(low_p, high_p):
                r2_low = max_r_sq * (low_p**2)
                r2_high = max_r_sq * (high_p**2)
                mask_band = (dist_sq >= r2_low) & (dist_sq <= r2_high)
                vals = curr_spec[mask_band]
                return torch.mean(vals).item() if vals.numel() > 0 else 0.0

            feat_fft_mid = get_energy(0.10, 0.30)
            feat_fft_high = get_energy(0.30, 0.70)
            
            valid_pix = input_tensor[i, 0][curr_mask > 0.5]
            feat_bright = torch.mean(valid_pix).item() if valid_pix.numel() > 0 else 0
            
            features_batch.append([feat_peak, feat_mean, feat_std, feat_fft_mid, feat_fft_high, feat_bright])
            
        return features_batch

def process_single_image_cpu_part(args):
    """CPU ä»»åŠ¡ï¼šè¯»å›¾ + è£åˆ‡"""
    file_path, detector = args
    try:
        full_img = load_best_available_image(file_path)
        if full_img is None: return None
        roi, _ = detector.detect_and_crop(full_img, standard_size=ROI_STANDARD_SIZE)
        return roi
    except: return None

def select_folder():
    root = tk.Tk(); root.withdraw()
    path = filedialog.askdirectory(title="é€‰æ‹©åŒ…å« labels.csv çš„æ–‡ä»¶å¤¹")
    root.destroy()
    return path

def main():
    print("=== AI æ¨¡å‹è®­ç»ƒå·¥å…· (GPU æ‰¹é‡æé€Ÿç‰ˆ) ===")
    
    folder = select_folder()
    if not folder: return
    
    csv_file = os.path.join(folder, "labels.csv")
    if not os.path.exists(csv_file):
        print("æœªæ‰¾åˆ° labels.csv")
        return

    # 1. è¯»å– CSV æ ‡ç­¾
    print("æ­£åœ¨è¯»å–æ ‡ç­¾...")
    data_entries = [] # list of (filename, label_idx)
    
    with open(csv_file, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        next(reader, None)
        for row in reader:
            if not row or len(row) < 2: continue
            try:
                raw_label = int(row[1])
                # æ˜ å°„: 1->Trash(0), 2->Soft(1), 3->Perfect(2)
                if raw_label == 1: cls = 0
                elif raw_label == 2: cls = 1
                elif raw_label >= 3: cls = 2
                else: continue
                data_entries.append((row[0], cls))
            except: continue

    print(f"æœ‰æ•ˆæ ·æœ¬æ•°: {len(data_entries)}")
    
    # 2. æ‰¹é‡ç‰¹å¾æå–
    detector = BirdDetector()
    gpu_extractor = GPUBatchFeatureExtractor()
    
    X = []
    y = []
    filenames = []
    
    # è¿›åº¦æ¡
    pbar = tqdm(total=len(data_entries), desc="GPU Feature Extraction")
    
    # åˆ†æ‰¹æ¬¡å¤„ç†
    for i in range(0, len(data_entries), BATCH_SIZE):
        batch_slice = data_entries[i : i + BATCH_SIZE]
        batch_files = [item[0] for item in batch_slice]
        batch_labels = [item[1] for item in batch_slice]
        batch_paths = [os.path.join(folder, f) for f in batch_files]
        
        # A. å¹¶è¡Œ IO & Crop
        rois = []
        with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:
            futures = [executor.submit(process_single_image_cpu_part, (path, detector)) for path in batch_paths]
            rois = [f.result() for f in futures]
            
        # B. GPU æ‰¹é‡æå–
        feats_list = gpu_extractor.extract_batch(rois)
        
        # C. æ”¶é›†ç»“æœ
        for j, feats in enumerate(feats_list):
            if feats is not None:
                X.append(feats)
                y.append(batch_labels[j])
                filenames.append(batch_files[j])
                
        pbar.update(len(batch_slice))
    
    pbar.close()
    
    X = np.array(X)
    y = np.array(y)
    
    print(f"\næœ€ç»ˆè®­ç»ƒé›†: {len(X)} å¼ ")
    print(f"Trash: {np.sum(y==0)}, Soft: {np.sum(y==1)}, Perfect: {np.sum(y==2)}")
    
    # 3. è®­ç»ƒæ¨¡å‹ (éšæœºæ£®æ—ç½‘æ ¼æœç´¢)
    print("\næ­£åœ¨è®­ç»ƒæ¨¡å‹ (Random Forest Grid Search)...")
    
    X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(
        X, y, range(len(filenames)), test_size=0.2, random_state=42
    )
    
    rf = RandomForestClassifier(random_state=42)
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'max_features': ['sqrt', 'log2']
    }
    
    grid_search = GridSearchCV(
        estimator=rf, 
        param_grid=param_grid, 
        cv=5, 
        n_jobs=-1,
        verbose=1,
        scoring='f1_macro'
    )
    
    grid_search.fit(X_train, y_train)
    
    print("-" * 60)
    print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
    
    best_clf = grid_search.best_estimator_
    y_pred = best_clf.predict(X_test)
    
    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='macro')
    
    print(f"ğŸ† æµ‹è¯•é›†å‡†ç¡®ç‡: {acc:.4f} | F1 Score: {f1:.4f}")
    print(classification_report(y_test, y_pred, target_names=CLASS_NAMES))
    
    # ä¿å­˜
    model_path = "best_bird_model_multiclass.pkl"
    joblib.dump(best_clf, model_path)
    print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜ä¸º: {model_path}")
    
    # 4. é”™è¯¯åˆ†æ
    print("\nç”Ÿæˆé”™è¯¯æŠ¥å‘Š...")
    errors = []
    final_pred = best_clf.predict(X_test)
    test_files = [filenames[i] for i in idx_test]
    
    for i in range(len(y_test)):
        if y_test[i] != final_pred[i]:
            true_l = CLASS_NAMES[y_test[i]]
            pred_l = CLASS_NAMES[final_pred[i]]
            severity = "ä¸¥é‡" if abs(y_test[i] - final_pred[i]) == 2 else "è½»å¾®"
            errors.append({
                'Filename': test_files[i],
                'True': true_l,
                'Predicted': pred_l,
                'Severity': severity
            })
            
    if errors:
        try:
            with open("error_analysis_gpu.csv", 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=['Filename', 'True', 'Predicted', 'Severity'])
                writer.writeheader()
                writer.writerows(errors)
            print(f"âš ï¸ {len(errors)} ä¸ªè¯¯åˆ¤å·²å†™å…¥ error_analysis_gpu.csv")
        except: pass
    else:
        print("ğŸ‰ æµ‹è¯•é›†æ— è¯¯åˆ¤ï¼")

if __name__ == "__main__":
    import multiprocessing
    multiprocessing.freeze_support()
    main()